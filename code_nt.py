# -*- coding: utf-8 -*-
"""Code_NT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AVd2xcdfuLnOaiND0nrqUZptSIR9uggH
"""



"""**PROJET CLUSTERING**

**Nicolas THORR**

Pour pouvoir exécuter les commandes, il faut télécharger le jeu de données sous le nom 'communities.data' à partir du site de l'UCI.

On le télécharge dans Colab, à partir de l onglet Fichier, à gauche.

Il n'est pas nécessaire d'exécuter de nouveau les commandes (notamment pour l'entraînement des modèles qui prend du temps). Si on le souhaite tout de même, on clique sur le bouton play à gauche de chaque commande/cellule. Il faut le faire depuis la première commande.

# Importation de la base de données et des bibliothèques
"""

# Afin de réaliser l’apprentissage automatique, j’utilise des bibliothèques dédiées
# et très utilisées sur Python, il s'agit notamment de Pandas.

import pandas as pd

# Pour la visualisation
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# J'importe la base de données sous le nom "df" ainsi que le nom des colonnes
# Je précise que les valeurs manquantes du fichier sont signalées par un point d'intérogation (?)
df = pd.read_csv('communities.data',header=None,na_values="?")
headers = ['state','county','community','communityname','fold','population','householdsize','racepctblack','racePctWhite','racePctAsian','racePctHisp','agePct12t21','agePct12t29','agePct16t24','agePct65up','numbUrban','pctUrban','medIncome','pctWWage','pctWFarmSelf','pctWInvInc','pctWSocSec','pctWPubAsst','pctWRetire','medFamInc','perCapInc','whitePerCap','blackPerCap','indianPerCap','AsianPerCap','OtherPerCap','HispPerCap','NumUnderPov','PctPopUnderPov','PctLess9thGrade','PctNotHSGrad','PctBSorMore','PctUnemployed','PctEmploy','PctEmplManu','PctEmplProfServ','PctOccupManu','PctOccupMgmtProf','MalePctDivorce','MalePctNevMarr','FemalePctDiv','TotalPctDiv','PersPerFam','PctFam2Par','PctKids2Par','PctYoungKids2Par','PctTeen2Par','PctWorkMomYoungKids','PctWorkMom','NumIlleg','PctIlleg','NumImmig','PctImmigRecent','PctImmigRec5','PctImmigRec8','PctImmigRec10','PctRecentImmig','PctRecImmig5','PctRecImmig8','PctRecImmig10','PctSpeakEnglOnly','PctNotSpeakEnglWell','PctLargHouseFam','PctLargHouseOccup','PersPerOccupHous','PersPerOwnOccHous','PersPerRentOccHous','PctPersOwnOccup','PctPersDenseHous','PctHousLess3BR','MedNumBR','HousVacant','PctHousOccup','PctHousOwnOcc','PctVacantBoarded','PctVacMore6Mos','MedYrHousBuilt','PctHousNoPhone','PctWOFullPlumb','OwnOccLowQuart','OwnOccMedVal','OwnOccHiQuart','RentLowQ','RentMedian','RentHighQ','MedRent','MedRentPctHousInc','MedOwnCostPctInc','MedOwnCostPctIncNoMtg','NumInShelters','NumStreet','PctForeignBorn','PctBornSameState','PctSameHouse85','PctSameCity85','PctSameState85','LemasSwornFT','LemasSwFTPerPop','LemasSwFTFieldOps','LemasSwFTFieldPerPop','LemasTotalReq','LemasTotReqPerPop','PolicReqPerOffic','PolicPerPop','RacialMatchCommPol','PctPolicWhite','PctPolicBlack','PctPolicHisp','PctPolicAsian','PctPolicMinor','OfficAssgnDrugUnits','NumKindsDrugsSeiz','PolicAveOTWorked','LandArea','PopDens','PctUsePubTrans','PolicCars','PolicOperBudg','LemasPctPolicOnPatr','LemasGangUnitDeploy','LemasPctOfficDrugUn','PolicBudgPerPop','ViolentCrimesPerPop']
df.columns = headers
# On obtient les premières lignes de la base.
df.head()

#Il y a 1994 individus et 128 variables dans le jeu de données

np.shape(df)

"""# Densité et corrélation"""

# Observons tout d'abors la distribution de la variable 'ViolentCrimesPerPop' dans le jeu de données
fig = plt.figure(figsize = (10, 4))

ax1 = fig.add_subplot(1,1,1)
ax1.set_xlabel("Variable ViolentCrimesPerPop")
ax1.set_ylabel("Frequence") 
sns.distplot(df['ViolentCrimesPerPop'], hist=True, kde=True, 
             bins=int(30), color = 'blue', 
             hist_kws={'edgecolor':'black'},
             kde_kws={'linewidth': 1})



corr = df.corr()

correlations = abs(corr['ViolentCrimesPerPop'])
var_int = correlations[correlations>0.45]
var_int = var_int.drop(labels=('ViolentCrimesPerPop'))
var_int = var_int.sort_values(ascending=False)
var_int





"""# Pré-traitement de la base de données"""

# Le site informe que 5 variables sont non prédictives.
# Et cela se comprend bien.
# Par exemple, le nom de la ville n'est pas utile dans notre cas.
# Enlevons les colonnes correspondantes du jeu de données.

df.drop(labels=['state','county','community','communityname','fold'],axis = 'columns',inplace=True)

# Je regarde les variables qui comporte des données manquantes.
# Pour chacune d'entre elle, je précise le nombre de valeurs manquantes.

nulls = df.isnull().sum()
print(nulls[nulls>0])
print()
print('Nombre de variables ayant des NA :', len(nulls[nulls>0]))

# Rappel : il y a 1994 observations.
# Ainsi, plus de 80 % des donées manquent pour certaines variables.

# On prend la décision de supprimer ces 23 variables qui comportent trop de valeurs manquantes.
# La variable 'OtherPerCap' ne comportant qu'une valeur manquante, on supprime la ligne correspondante.

to_drop = nulls[nulls>0]
df = df.dropna(axis=1,subset=to_drop)
ligne = df[df['OtherPerCap'].isnull() == True]
df = df.drop(ligne.index[0])
df

# On obtient un jeu de données sans valeurs manquantes qui est donc exploitable.
# Il reste maintenant 101 variables, soit 100 variables prédictives.

np.shape(df)



"""# Création d'une forêt aléatoire, entraînement et mesure de performance"""

# On importe de Scikitlearn les outils utiles pour créer le modèle, l'entraîner et l'évaluer.

# Pour séparer le jeu de données en ensemble d'apprentissage et en ensemble de test
# en mélangeant la bae de données et en retenant une certaine proportion des instances pour chaque ensemble.
from sklearn.model_selection import train_test_split

# Pour mesurer la performance des modèle en utilisant la cross-validation :
from sklearn.model_selection import cross_val_score

# Pour optimiser les paramètres du modèle :
from sklearn.model_selection import GridSearchCV

# Pour créer un modèle de forêt aléatoire :
from sklearn.ensemble import RandomForestRegressor as rfr



# On isole dans y la variable à prédire, qui est 'ViolentCrimesPerPop'
# Ainsi X contiendra les variables prédictives.

y = df['ViolentCrimesPerPop']
X = df.drop(['ViolentCrimesPerPop'],axis=1)

# On importe plusieurs métriques permettant d'évaluer le modèle
# Il s'agit de l'erreur absolue moyenne, la variance et le r^2.

import sklearn
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import r2_score as r2

# On divise la base de données en un ensemble de test et un ensemble d'entraînement.

X_train, X_test, y_train, y_test = train_test_split(X,y)

# On précise les paramètres du modèle de forêt aléatoire


model = rfr(n_estimators=600, criterion='mse', max_depth=10, max_features=20, min_samples_split=4,
            max_leaf_nodes=None, bootstrap=True, min_impurity_decrease=0)

"""Voici la définition des paramètres principaux du modèle :

**n-estimators** : nombre d'arbres à générer

**criterion** : la métrique, mesure de mélange

**max_depth** : profondeur maximale des arbres de régression

**max_features** : nombre de variables à utiliser dans chaque arbre
"""

# On entraîne/fitte le modèle grâce à l'ensemble d'entraînement.

model.fit(X_train,y_train)

# L'entraînement étant fait, nous avons construit une forêt aléatoire.
# On prédit alors la valeur de la variable 'ViolentCrimesPerPop' sur les individus de l'ensemble de test.
# i.e. des individus que le modèle n'a pas vus.

pred = model.predict(X_test)

# On peut maintenant comparer les prédictions aux vraies valeurs
# On utilise plusieurs mesures pour mesurer la performance du modèle.
aerr = mae(pred,y_test)
print('Erreur absolue :' , aerr)

serr = mse(pred,y_test)
print('Erreur quadratique', serr)

rsq = r2(pred,y_test)
print('r^2', rsq)

# On peut aussi évaluer le modèle par cross-validation.
# Ici pour le R^2 avec 5 folds.
cross_val_score(model,X,y,cv=5, scoring='r2').mean()



"""# Sélection de variables et perspectives"""

importance = [model.feature_importances_]
features = [X.columns]
desc_importance = sorted(importance, reverse=True)

feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')

X_modifie = X[['PctIlleg','PctKids2Par','PctFam2Par','NumIlleg','racePctWhite', 'racepctblack', 'HousVacant', 'PctPersDenseHous']]

X_train, X_test, y_train, y_test = train_test_split(X_modifie,y)

model = rfr(n_estimators=600, criterion='mse', max_depth=10, max_features=None, min_samples_split=4,
            max_leaf_nodes=None, bootstrap=True, min_impurity_decrease=0)


model.fit(X_train,y_train)

pred = model.predict(X_test)

aerr = mae(pred,y_test)
print('Erreur absolue :' , aerr)

serr = mse(pred,y_test)
print('Erreur quadratique', serr)

# Au lieu d'imposer les paramètres du modèle, on peut les optimiser avec la fonction GridSearchCV
# On va chercher le nombre d'arbres à construire, le nombre maximal de feuilles à chaque noeud, 
from sklearn.model_selection import GridSearchCV

def rfr_model(X, y):# Perform Grid-Search
    gsc = GridSearchCV(
        estimator=rfr(),
        param_grid={
            'max_depth': range(3,10),
            'n_estimators': (50, 100, 500), 'max_leaf_nodes':(2,3,5,7), 'max_features': (2,3,5,10,15)
        },
        cv=5, scoring='neg_mean_absolute_error', verbose=0, n_jobs=-1)
    
    grid_result = gsc.fit(X,y)
    best_params = grid_result.best_params_
    
    # On crée ensuite la forêt avec les meilleurs paramètres.

    rf_r = rfr(max_depth=best_params["max_depth"], n_estimators=best_params["n_estimators"], random_state=False, verbose=False, max_leaf_nodes=best_params['max_leaf_nodes'],
                                                                                                                                                           max_features= best_params['max_features'])# Perform K-Fold CV
    scores = cross_val_score(rf_r, X, y, cv=5, scoring='neg_mean_absolute_error')

    return rf_r, scores

final_model, final_scores = rfr_model(X,y)
final_scores
final_model.fit(X_train,y_train)